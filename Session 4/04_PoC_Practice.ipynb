{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsZPCLvU8aII"
   },
   "source": [
    "# Computer Vision Course #04\n",
    "## Train Deep Learning Model Practice\n",
    "by Can Nguyen - TIC Computer Vision Team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this practice, you will train your deep learning model for your PoC Practice, in Python using the Keras deep learning library.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "+ How to load your dataset in Keras.\n",
    "\n",
    "+ How to add data augmentation method for your dataset in Keras.\n",
    "\n",
    "+ How to implement and evaluate Convolutional Neural Network with Regularization techniques for your dataset.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELNGB9dw-u-J"
   },
   "outputs": [],
   "source": [
    "#Import library\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses a so-called data-generator for inputting data into the neural network, which will loop over the data for eternity.\n",
    "\n",
    "We have a small training-set so it helps to artificially inflate its size by making various transformations to the images. We use a built-in data-generator that can make these random transformations. This is also called an augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64,64)\n",
    "#Enter the path to your dataset\n",
    "train_dir= 'D:\\\\data_1\\\\UTKFace_dataset\\\\train'\n",
    "test_dir = 'D:\\\\data_1\\\\UTKFace_dataset\\\\test' #change \"valid\" to \"test\" if your dataset doesn't have \"valid\" folder\n",
    "\n",
    "#---------------------------\n",
    "\n",
    "\n",
    "\"\"\"Data generator for training dataset\"\"\"\n",
    "datagen_train = ImageDataGenerator(rescale=1./255,\n",
    "                                   horizontal_flip=True,\n",
    "                                   data_format = 'channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a data-generator for the test-set, but this should not do any transformations to the images because we want to know the exact classification accuracy on those specific images. So we just rescale the pixel-values so they are between 0.0 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data generator for test dataset\"\"\"\n",
    "datagen_test = ImageDataGenerator(rescale=1./255,\n",
    "                                 data_format= \"channels_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data-generators will return batches of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the randomly transformed images during training, so as to inspect whether they have been overly distorted, so we have to adjust the parameters for the data-generator above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    save_to_dir = None\n",
    "else:\n",
    "    save_to_dir='augmented_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the actual data-generator that will read files from disk, resize the images and return a random batch.\n",
    "\n",
    "It is somewhat awkward that the construction of the data-generator is split into these two steps, but it is probably because there are different kinds of data-generators available for different data-types (images, text, etc.) and sources (memory or disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train = datagen_train.flow_from_directory(directory=train_dir,\n",
    "                                                    target_size=input_shape,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    save_to_dir=save_to_dir)\n",
    "generator_test = datagen_test.flow_from_directory(directory=test_dir,\n",
    "                                                  target_size=input_shape,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wD572AQW8vFH"
   },
   "source": [
    "Define parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WRNhBDs-u-X"
   },
   "outputs": [],
   "source": [
    "### Some core parameters for training\n",
    "num_train = generator_train.n\n",
    "num_test = generator_test.n\n",
    "steps_test = num_test//batch_size\n",
    "steps_per_epoch = num_train // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFUu9YNj5G-B"
   },
   "source": [
    "Get the class-names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGDOBLIt-u-f"
   },
   "outputs": [],
   "source": [
    "### Re-check our dataset\n",
    "cls_train = generator_train.classes\n",
    "cls_test = generator_test.classes\n",
    "class_names = list(generator_train.class_indices.keys())\n",
    "print(class_names)\n",
    "num_classes = generator_train.num_classes\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjUWIcia8802"
   },
   "source": [
    "## Define your CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAsvFbAg4p2J"
   },
   "outputs": [],
   "source": [
    "# Create model \n",
    "model = Sequential()\n",
    "# Add convolution 2D shape (5x5x32),padding same, stride=1)\n",
    "\"Your code here:\"\n",
    "# Add Max_pooling 2D kernel size(2,2),no padding, stride=1\n",
    "\"Your code here:\"\n",
    "# Add convolution 2D shape (3x3x64),padding same, stride=1 \n",
    "\"Your code here:\"\n",
    "# Add Max_pooling 2D kernel size(2,2)\n",
    "\"Your code here:\"\n",
    "# ADD flatten \n",
    "\"Your code here:\"\n",
    "# ADD dense layer 512 hidden unit\n",
    "\"Your code here:\"\n",
    "# ADD dense layer\n",
    "model.add(Dense(num_classes, activation='softmax',name=\"Dense_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KrIKBhc6ruj"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-4)\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# Summary model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_0-eR7Z9GBv"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7TkNLcO6tTq"
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=generator_train,\n",
    "                              epochs=10,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data=generator_test,\n",
    "                              validation_steps=steps_test)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuary score :\",scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcHN9srx9IPu"
   },
   "source": [
    "## Show training history as a graph\n",
    "Keras records the performance metrics at the end of each \"epoch\" so they can be plotted later. This shows that the loss-value for the training-set generally decreased during training, but the loss-values for the test-set were a bit more erratic. Similarly, the classification accuracy generally improved on the training-set while it was a bit more erratic on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trYBlACK7pps"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Get the classification accuracy and loss-value\n",
    "    # for the training-set.\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "\n",
    "    # Get it for the validation-set (we only use the test-set).\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the accuracy and loss-values for the training-set.\n",
    "    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n",
    "    plt.plot(loss, 'o', color='b', label='Training Loss')\n",
    "    \n",
    "    # Plot it for the test-set.\n",
    "    plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n",
    "    plt.plot(val_loss, 'o', color='r', label='Test Loss')\n",
    "\n",
    "    # Plot title and legend.\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ensure the plot shows correctly.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFRnb4KZ7wAy"
   },
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ground-truth for your PoC, that contain the index and class name\n",
    "\n",
    "Ex:\n",
    "{0:\"Ford\", 1:'Lambor', 2: \"Range\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model):\n",
    "    # Load and resize the image using PIL.\n",
    "    # Use tensorflow to load image (Work with transparent image like png)\n",
    "    img = load_img(image_path, target_size=input_shape)\n",
    "    print(img)\n",
    "    # Plot the image.\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "#     print(img)\n",
    "    img_arr = img_to_array(img)/255.\n",
    "    img_arr = np.expand_dims(img_arr, axis=0)\n",
    "    print(img_arr.shape)\n",
    "    pred1 = model.predict(img_arr)\n",
    "    #pred = model.predict_classes(img_array)\n",
    "    label = np.argmax(pred1,axis=1)\n",
    "    print(label)\n",
    "    print(pred1)\n",
    "    return ground_truth[label[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('Path_to_your_image',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "PoC_Practice.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
